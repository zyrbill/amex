{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ccf6f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\envs\\local_nmt\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "886e6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_not_used():\n",
    "    # cid is the label encode of customer_ID\n",
    "    # row_id indicates the order of rows\n",
    "    return ['row_id', 'customer_ID', 'target', 'cid', 'S_2']\n",
    "    \n",
    "def preprocess(df):\n",
    "    df['row_id'] = np.arange(df.shape[0])\n",
    "    not_used = get_not_used()\n",
    "    cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120',\n",
    "                'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in not_used+cat_cols:\n",
    "            df[col] = df[col].round(2)\n",
    "\n",
    "    # compute \"after pay\" features\n",
    "    for bcol in [f'B_{i}' for i in [11,14,17]]+['D_39','D_131']+[f'S_{i}' for i in [16,23]]:\n",
    "        for pcol in ['P_2','P_3']:\n",
    "            if bcol in df.columns:\n",
    "                df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
    "\n",
    "    df['S_2'] = pd.to_datetime(df['S_2'])\n",
    "    df['cid'], _ = df.customer_ID.factorize()\n",
    "        \n",
    "    num_cols = [col for col in df.columns if col not in cat_cols+not_used]\n",
    "    \n",
    "    dgs = add_stats_step(df, num_cols)\n",
    "        \n",
    "    # cudf merge changes row orders\n",
    "    # restore the original row order by sorting row_id\n",
    "    df = df.sort_values('row_id')\n",
    "    df = df.drop(['row_id'],axis=1)\n",
    "    return df, dgs\n",
    "\n",
    "def add_stats_step(df, cols):\n",
    "    n = 50\n",
    "    dgs = []\n",
    "    for i in range(0,len(cols),n):\n",
    "        s = i\n",
    "        e = min(s+n, len(cols))\n",
    "        dg = add_stats_one_shot(df, cols[s:e])\n",
    "        dgs.append(dg)\n",
    "    return dgs\n",
    "\n",
    "def add_stats_one_shot(df, cols):\n",
    "    stats = ['mean','std']\n",
    "    dg = df.groupby('customer_ID').agg({col:stats for col in cols})\n",
    "    out_cols = []\n",
    "    for col in cols:\n",
    "        out_cols.extend([f'{col}_{s}' for s in stats])\n",
    "    dg.columns = out_cols\n",
    "    dg = dg.reset_index()\n",
    "    return dg\n",
    "\n",
    "def process_data(df):\n",
    "    df,dgs = preprocess(df)\n",
    "    df = df.drop_duplicates('customer_ID',keep='last')\n",
    "    for dg in dgs:\n",
    "        df = df.merge(dg, on='customer_ID', how='left')\n",
    "    diff_cols = [col for col in df.columns if col.endswith('_diff')]\n",
    "    df = df.drop(diff_cols,axis=1)\n",
    "    df = pd.get_dummies(df, columns=['D_63', 'D_64'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "65f12f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
      "C:\\Users\\Yiren Zhou\\AppData\\Local\\Temp\\ipykernel_30216\\1812420225.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['cid'], _ = df.customer_ID.factorize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8.95 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_pickle(\"data/train_data.pkl\").head(100000)\n",
    "train = process_data(train)\n",
    "train_labels = pd.read_pickle(\"data/train_labels.pkl\").head(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9ee71ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(train_labels, on='customer_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a0f20d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_train(x, y, xt, yt):\n",
    "    print(\"# of features:\", x.shape[1])\n",
    "    assert x.shape[1] == xt.shape[1]\n",
    "    dtrain = xgb.DMatrix(data=x, label=y)\n",
    "    dvalid = xgb.DMatrix(data=xt, label=yt)\n",
    "    params = {\n",
    "            'objective': 'binary:logistic', \n",
    "            #'tree_method': 'gpu_hist', \n",
    "            #'gpu_id': 0,\n",
    "            'max_depth': 7,\n",
    "            'subsample':0.88,\n",
    "            'colsample_bytree': 0.5,\n",
    "            'gamma':1.5,\n",
    "            'min_child_weight':8,\n",
    "            'lambda':70,\n",
    "            'eta':0.03,\n",
    "    }\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    bst = xgb.train(params, dtrain=dtrain,\n",
    "                num_boost_round=2600,evals=watchlist,\n",
    "                early_stopping_rounds=500, feval=xgb_amex, maximize=True,\n",
    "                verbose_eval=100)\n",
    "    print('best ntree_limit:', bst.best_ntree_limit)\n",
    "    print('best score:', bst.best_score)\n",
    "    return bst.predict(dvalid, iteration_range=(0,bst.best_ntree_limit)), bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9f9e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "# Created by https://www.kaggle.com/yunchonggan\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "# we still need the official metric since the faster version above is slightly off\n",
    "import pandas as pd\n",
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b0f7762f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1c6477b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features: 591\n",
      "[21:59:29] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.67685\ttrain-amex:0.65852\teval-logloss:0.67683\teval-amex:0.61735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\envs\\local_nmt\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain-logloss:0.25192\ttrain-amex:0.81772\teval-logloss:0.27062\teval-amex:0.74518\n",
      "[200]\ttrain-logloss:0.19863\ttrain-amex:0.86226\teval-logloss:0.23825\teval-amex:0.76309\n",
      "[300]\ttrain-logloss:0.17398\ttrain-amex:0.89767\teval-logloss:0.23074\teval-amex:0.76731\n",
      "[400]\ttrain-logloss:0.15653\ttrain-amex:0.92321\teval-logloss:0.22778\teval-amex:0.77287\n",
      "[500]\ttrain-logloss:0.14298\ttrain-amex:0.94012\teval-logloss:0.22688\teval-amex:0.76738\n",
      "[600]\ttrain-logloss:0.13187\ttrain-amex:0.95496\teval-logloss:0.22711\teval-amex:0.77282\n",
      "[700]\ttrain-logloss:0.12318\ttrain-amex:0.96449\teval-logloss:0.22807\teval-amex:0.77343\n",
      "[800]\ttrain-logloss:0.11592\ttrain-amex:0.97384\teval-logloss:0.22865\teval-amex:0.77517\n",
      "[900]\ttrain-logloss:0.11013\ttrain-amex:0.97833\teval-logloss:0.22923\teval-amex:0.77307\n",
      "[1000]\ttrain-logloss:0.10502\ttrain-amex:0.98141\teval-logloss:0.23001\teval-amex:0.77195\n",
      "[1100]\ttrain-logloss:0.10086\ttrain-amex:0.98551\teval-logloss:0.23078\teval-amex:0.76897\n",
      "[1200]\ttrain-logloss:0.09735\ttrain-amex:0.98803\teval-logloss:0.23135\teval-amex:0.76875\n",
      "[1300]\ttrain-logloss:0.09463\ttrain-amex:0.98861\teval-logloss:0.23167\teval-amex:0.76681\n",
      "[1316]\ttrain-logloss:0.09424\ttrain-amex:0.98894\teval-logloss:0.23179\teval-amex:0.76673\n",
      "best ntree_limit: 817\n",
      "best score: 0.777016\n",
      "Fold 0 amex 0.7770\n",
      "# of features: 591\n",
      "[22:00:14] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.67683\ttrain-amex:0.64082\teval-logloss:0.67684\teval-amex:0.59492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\envs\\local_nmt\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain-logloss:0.24962\ttrain-amex:0.81675\teval-logloss:0.27105\teval-amex:0.76013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "Input \u001b[1;32mIn [77]\u001b[0m, in \u001b[0;36mxgb_train\u001b[1;34m(x, y, xt, yt)\u001b[0m\n\u001b[0;32m      6\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m#'tree_method': 'gpu_hist', \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.03\u001b[39m,\n\u001b[0;32m     17\u001b[0m }\n\u001b[0;32m     18\u001b[0m watchlist \u001b[38;5;241m=\u001b[39m [(dtrain, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m), (dvalid, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m---> 19\u001b[0m bst \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwatchlist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxgb_amex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest ntree_limit:\u001b[39m\u001b[38;5;124m'\u001b[39m, bst\u001b[38;5;241m.\u001b[39mbest_ntree_limit)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest score:\u001b[39m\u001b[38;5;124m'\u001b[39m, bst\u001b[38;5;241m.\u001b[39mbest_score)\n",
      "File \u001b[1;32mC:\\Python\\envs\\local_nmt\\lib\\site-packages\\xgboost\\training.py:188\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(params, dtrain, num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, evals\u001b[38;5;241m=\u001b[39m(), obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    116\u001b[0m           maximize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, evals_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    117\u001b[0m           verbose_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xgb_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;124;03m\"\"\"Train a booster with given parameters.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m    Booster : a trained booster model\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m     bst \u001b[38;5;241m=\u001b[39m \u001b[43m_train_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxgb_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bst\n",
      "File \u001b[1;32mC:\\Python\\envs\\local_nmt\\lib\\site-packages\\xgboost\\training.py:81\u001b[0m, in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python\\envs\\local_nmt\\lib\\site-packages\\xgboost\\core.py:1680\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(dtrain)\n\u001b[0;32m   1679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1680\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1681\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1682\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1684\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "not_used = get_not_used()\n",
    "not_used = [i for i in not_used if i in train.columns]\n",
    "msgs = {}\n",
    "folds = 4\n",
    "score = 0\n",
    "\n",
    "for i in range(folds):\n",
    "    mask = train['cid']%folds == i\n",
    "    tr,va = train[~mask], train[mask]\n",
    "    \n",
    "    x, y = tr.drop(not_used, axis=1), tr['target']\n",
    "    xt, yt = va.drop(not_used, axis=1), va['target']\n",
    "    yp, bst = xgb_train(x, y, xt, yt)\n",
    "    bst.save_model(f'xgb_{i}.json')\n",
    "    amex_score = amex_metric(pd.DataFrame({'target':yt.values}), \n",
    "                                    pd.DataFrame({'prediction':yp}))\n",
    "    msg = f\"Fold {i} amex {amex_score:.4f}\"\n",
    "    print(msg)\n",
    "    score += amex_score\n",
    "score /= folds\n",
    "print(f\"Average amex score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5f98c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_iter(path, chunks=4):\n",
    "    \n",
    "    test_rows = 11363762\n",
    "    chunk_rows = test_rows // chunks\n",
    "    \n",
    "    test = pd.read_parquet(f'{path}/test.parquet',\n",
    "                             columns=['customer_ID','S_2'],\n",
    "                             num_rows=test_rows)\n",
    "    test = get_segment(test)\n",
    "    start = 0\n",
    "    while start < test.shape[0]:\n",
    "        if start+chunk_rows < test.shape[0]:\n",
    "            end = test['cus_count'].values[start+chunk_rows]\n",
    "        else:\n",
    "            end = test['cus_count'].values[-1]\n",
    "        end = int(end)\n",
    "        df = pd.read_parquet(f'{path}/test.parquet',\n",
    "                               num_rows = end-start, skiprows=start)\n",
    "        start = end\n",
    "        yield process_data(df)\n",
    "        \n",
    "def get_segment(test):\n",
    "    dg = test.groupby('customer_ID').agg({'S_2':'count'})\n",
    "    dg.columns = ['cus_count']\n",
    "    dg = dg.reset_index()\n",
    "    dg['cid'],_ = dg['customer_ID'].factorize()\n",
    "    dg = dg.sort_values('cid')\n",
    "    dg['cus_count'] = dg['cus_count'].cumsum()\n",
    "    \n",
    "    test = test.merge(dg, on='customer_ID', how='left')\n",
    "    test = test.sort_values(['cid','S_2'])\n",
    "    assert test['cus_count'].values[-1] == test.shape[0]\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4fa65616",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:4\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cids = []\n",
    "yps = []\n",
    "chunks = 4\n",
    "for df in tqdm(load_test_iter('data',chunks),total=chunks):\n",
    "    cids.append(df['customer_ID'])\n",
    "    not_used = [i for i in not_used if i in df.columns]\n",
    "\n",
    "    yp = 0\n",
    "    for i in range(folds):\n",
    "        bst = xgb.Booster()\n",
    "        bst.load_model(f'xgb_{i}.json')\n",
    "        dx = xgb.DMatrix(df.drop(not_used, axis=1))\n",
    "        print('best ntree_limit:', bst.best_ntree_limit)\n",
    "        yp += bst.predict(dx, iteration_range=(0,bst.best_ntree_limit))\n",
    "    yps.append(yp/folds)\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "df['customer_ID'] = pd.concat(cids)\n",
    "df['prediction'] = np.concatenate(yps)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eadbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (local_nmt)",
   "language": "python",
   "name": "local_nmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
